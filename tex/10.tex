\setcounter{chapter}{9}
\chapter{Реализация Haskell в GHC}

На момент написания этой книги основным компилятором
Haskell является GHC. Остальные конкуренты отстают очень
сильно. Отметим компилятор Hugs (его хорошо использовать 
для демонстрации Haskell на чужом компьютере, если вы не хотите
устанавливать тяжёлый GHC).
В этой главе мы обзорно рассмотрим как язык Hаskell 
реализован в GHC. 
GHC -- как ни парадоксально это звучит, это самая успешная
программа написанная на Haskell. GHC уже двадцать лет.
Отметим основных разработчиков. Это Саймон Пейтон Джонс
(Simon Peyton Jones) и Саймон Марлоу (Simon Marlow).

GHC состоит из трёх частей. Это сам компилятор, основные
библиотеки языка (такие как Prelude) и низкоуровневая 
система вычислений (она отвечает за управление памятью, 
потоками, вычисление примитивных операций). 
Весь GHC кроме системы вычислений
написан на Haskell. Система вычислений написана на C.
Компилятор принимает набор файлов с исходным кодом 
(а также возможно объектных и интерфейсных файлов) 
и генерирует код низкого уровня. Система вычислений низкого уровня
используется в этом коде как библиотека. Она статически 
подключается к любому нативному коду, который генерируется GHC. 
Далее мы сосредоточимся на изучении компилятора. 

Но перед этим давайте освежим в памяти (или узнаем)
несколько терминов. У нас есть код на Haskell, что значит
перевести в код низкого уровня? Код низкого уровня представляет
собой набор инструкций, которые изменяют значения в памяти
компьютера. Изменение значений происходит с помощью 
базовых операций, которые выполняются в процессоре компьютера.
Память компьютера представляет собой ленту ячеек. У каждой
ячейки есть адрес и содержание. По адресу мы можем читать
данные из ячейки и записывать их туда. Эти операции также
выполняются с помощью инструкций. 
Мы будем делить память на стек (stack), кучу (heap) и 
регистры (registers). 

Стек~-- это очередь с 
принципом работы \Quote{последним пришёл, первым ушёл}.
Стек можно представить как стопку книг. У нас есть две
операции: положить книгу наверх, и снять верхнюю книгу.
Стек очень удобен для переключения контекстов вычисления.
Представьте, что у нас есть функция, которая внутри 
вызывает другую функцию, а та следующую. Находясь в 
верхней функции при заходе во вторую мы сохраняем контекст
внешней функции в стеке. Контекст -- это та информация, которая
нужна нам для того, чтобы продолжить вычисления. 
Как только мы доходим до третьей функции, мы \Quote{кладём на стопку
сверху} контекст второй функции, как только третья функция вычислена,
мы обращаемся к стеку и снимаем с него контекст второй функции
продолжаем вычислять и как только вторая функция заканчивается
снова обращаемся к стеку. А там сверху уже лежит контекст 
самой первой функции. Мы можем продолжать вычисления. Так 
происходит вычисление вложенных функций в императивных 
языках программирования. 

В куче мы храним разные данные. Данные бывают статическими
(они нужны нам на протяжении выполнения всей программы)
и динамическими (время жизни динамических данных заранее
неизвестно, например это могут быть отложенные вычисления,
мы не знаем когда ни нам понадобятся).
У кучи также две операции: выделить блок памяти, эта операция
принимает размер блока и возвращает адрес, по которому
удалось выделить память, и освободить память по указанному адресу.
Регистры находятся в процессоре. В них можно записывать и читать
данные, при этом операции обращения к регистрам будут 
происходить очень быстро.

Посмотрим как GHC справляется с переводом процесса редукции 
синонимов на язык понятный нашему компьютеру. Язык обновления стека и кучи.
Это большая и трудная глава, читайте не спеша. Если покажется
совсем трудно~-- пропустите, вернётесь потом, когда захочется
писать не только красивые, но и эффективные программы.

\section{Этапы компиляции}

Рассмотрим этапы компиляции программы (\RefFig{CompilerStages}).

\newcommand{\Dia}[1]{\fbox{\text{#1}}}

\begin{figure}[ht]
\centering
\begin{diagram}
 &          & \text{Файл .hs} \\
 &          & \dTo  \\
 &          & \Dia{Построение синтаксического дерева} \\
 &          & \dTo \\
 &          & \Dia{Разрешение имён} \\
 &          & \dTo \\
 &          & \Dia{Проверка типов} \\
 &          & \dTo \\
 &          & \Dia{Устранение синтаксического сахара} \\
 &          & \dTo^{\text{Core}} \\
 &          & \Dia{Упрощение Core}     & \rTo & \Dia{Генерация кода для ghci} \\
 &          & \dTo^{\text{STG}} \\
 &          & \Dia{Генерация Cmm} \\ 
 &          & \dTo \\
 &          & \Dia{Cmm}    &        &  \\
 & \ldTo    & \dTo          & \rdTo  &  \\
\Dia{C} &  & \Dia{Native} &        & \Dia{LLVM}  \\
\end{diagram}
\caption{Этапы компиляции}
\label{fig:CompilerStages}
\end{figure}

На первых трёх этапах происходит проверка ошибок. 
Сначала мы строим синтаксическое дерево программы. 
Если мы нигде не забыли скобки, не ошиблись в простановке 
ключевых слов, то этот этап успешно завершится. 
Далее мы приписываем ко всем функциям их полные имена.
Дописываем перед всеми определениями имя модуля, в котором они
определены. Обычно на этом этапе нам сообщают о том, что
мы забыли определить какую-нибудь функцию, часто это связано
с простой опечаткой. Следующий этап -- самый важный. 
Происходит вывод типов для всех значений и проверка
программы по типам. Блок кода, отвечающий за проверку типов,
является самым большим в GHC. Haskell имеет очень 
развитую систему типов. Многих возможностей мы ещё 
не коснулись, часть из них мы рассмотрим в главе 17.
Допустим, что мы исправили все ошибки связанные с типами,
тогда компилятор начнёт переводить Haskell в Core.

Core -- это функциональный язык программирования, который 
является сильно урезанной версией Haskell. Помните мы говорили, что 
в Haskell поддерживается несколько стилей (композиционный
и декларативный). Что хорошо для программиста, не очень хорошо 
для компилятора. Компилятор устраняет весь синтаксический сахар
и выражает все определения через простейшие конструкции языка Core.
Далее происходит серия оптимизаций языка Core. На дереве
описания программы выполняется серия функций типа  \mbox{\In{Core -> Core}}.
Например происходит замена вызовов коротких функций на 
их правые части урвнений (встраивание или inlining), 
выражения, которые проводят декомпозицию в \In{case}-выражениях 
по константам, заменяются на соответствующие этим константам 
выражения. По требованию GHC может провести анализ строгости
(strictness analysis). Он заключается в том, что GHC
ищет аргументы функций, которые могут быть вычислены 
более эфективно с помощью вычисления по значению и
расставляет анотации строгости. И многие многие другие
оптимизации кода. Все они представлены в виде преобразования
синтаксического дерева программы. Также этот этап называют
упрощением программы. 

После этого Core переводится на STG. Это функциональный
язык, повторяющий Core. Он содержит дополнительную информацию,
которая необходима низкоуровневым бибилиотекам на этапе 
вычисления программы. Затем из STG генерируется код
языка \verb!C--!. Это язык низкого уровня, \Quote{портируемый ассемблер}.
На этом языке не пишут программы, он предназначен для 
автоматической генерации кода. Далее из него получают
другие низкоуровневые коды. Возможна генерация C, LLVM
и нативного кода (код, который исполняется операционной системой). 

\section{Язык STG}

STG расшифровывается как Spineless Tagless G-machine. 
G-machine или Г-машина -- это низкоуровневое описание 
процесса редукции графов (от Graph). Пока мы называли этот процесс
редукцией синонимов. Spineless и Tagless -- это термины специфичные
для G-машины, которая была придумана разработчиками GHC.
Tagless относится к особому представлению объектов в куче
(объекты представлены единообразно, так что им не нужен специальный
тег для обозначения типа объекта), а Spineless относится
к тому, что в отличие от машин-предшественников, которые
описывают процесс редукции графов виде последовательности
инструкций, STG является небольшим функциональным языком. 
На \RefFig{SyntaxSTG} представлен синтаксис языка STG. 
Синтаксис упрощён для чтения людьми. Несмотря на упрощения 
мы сможем посмотреть как происходит вычисление выражений. 
    
\begin{figure}[ht]
\centering
\[\begin{array}{rrlll}
\text{Переменные}   & x,y,f,g   &       &   &  \\
\text{Конструкторы} & C         &       &   & 
    \text{Объявлены в определениях типов}  \\
\text{Литералы}     & lit       & ::=   & i \Or d & 
    \text{Незапакованные целые} \\
 & & & & \text{или действительные числа} \\
\text{Атомы}        & a, v      & ::=   & lit \Or x & 
    \text{Аргументы функций атомарны} \\
\text{Арность функции} & k      & ::=   & \bullet   & 
    \text{Арность неизвестна} \\
                       &        & \Or   & n         & 
    \text{Арность известна } n \geq 1 \\
\\
\text{Выражения}       &   e     & ::=  & a         & \text{Атом} \\
                       &         & \Or  & f^k\ a_1 \dots a_n & 
    \text{Вызов функции } (n \geq 1) \\ 
                       &         & \Or  & \oplus\ a_1 \dots a_n & 
    \text{Вызов примитивной функции } (n \geq 1) \\  
\\
                       &         & \Or  & 
            \texttt{let}\ x\ \texttt{=}\ obj\ \texttt{in}\ e  & 
    \text{Выделение нового объекта } obj \text{ в куче} \\ 
                       &         & \Or  & 
            \texttt{case } e \texttt{ of } \{ alt_1; \dots ;alt_n \}  & 
    \text{Приведение выражения } e \text{ к СЗНФ} \\  
\\
\text{Альтернативы}    & alt     & ::=  & C\ x_1 \dots x_n \Ra e &
    \text{Сопоставление с образцом } (n \geq 1) \\
                       &         & \Or  & x \Ra e &
    \text{Альтернатива по умолчанию} \\
\\
\text{Объекты в куче}  & obj     & ::=  & FUN(x_1 \dots x_n \Ra e)  &
    \text{Функция арности } n \geq 1 \\
                        &        & \Or  & PAP(f\ a_1 \dots a_n) &
    \text{Частичное применение } f \text{ может} \\
 & & & & \text{указывать только на } FUN \\
                        &        & \Or  & CON(C\ a_1 \dots a_n) &
        \text{Полное применение конструктора } (n \geq 0) \\
                        &        & \Or  & THUNK\ e  & 
        \text{Отложенное вычисление}  \\   
                        &        & \Or  & BLACKHOLE &
        \text{Используется только во время} \\
 & & & & \text{выполнения программы} \\
\text{Программа}        & prog  & ::=   & 
    f_1 \texttt{=} obj_1\ ; \dots ;\ f_n \texttt{=} obj_n & \\

\end{array}\]
\caption{Синтаксис STG}
\label{fig:SyntaxSTG}
\end{figure}

По синтаксису STG можно понять, какие выражения языка
Haskell являются синтаксическим сахаром. Им просто нет 
места в языке STG. Например, не видим мы сопоставления с образцом.
Оно как и \In{if}-выражения переписывается через \In{case}-выражения.
Исчезли \In{where}-выражения. Конструкторы могут применяться
только полностью, то есть для применения конструктора мы должны
передать ему все аргументы. В STG \In{let}-выражения
разделяют на не рекурсивные (\In{let}) и рекурсивные (\In{letrec}).
Разделение проводится в целях оптимизации, мы же будем
считать, что эти случаи описываются одной конструкцией. 

На что стоит обратить внимание? Заметим, что функции
могут принимать только атомарные значения 
(либо примитивные значения, либо переменные). 
В данном случае переменные указывают на объекты в куче. 
Так если в Haskell мы пишем:

\begin{code}
foldr f (g x y) (h x)
\end{code}

В STG это выражение примет вид:

\begin{code}
let gxy = THUNK (g x y)
    hx  = THUNK (h x)
in  foldr f gxy hx
\end{code}

У функций появились степени. Что это? Степени указывают
на арность функции, то есть на количество принимаемых аргументов.
Количество принимаемых аргументов определяется по левой части
функции. Поскольку в Haskell функции могут возвращать другие функции,
очень часто мы не можем знать арность, тогда мы пишем $\bullet$.

Отметим два важных принципа вычисления на STG:

\begin{itemize}
\item Новые объекты создаются в куче \emph{только} в \In{let}-выражениях
\item Выражение приводится к СЗНФ \emph{только} в \In{case}-выражениях
\end{itemize}

Выражение \In{let a = obj in e} означает
добавь в кучу объект \In{obj} под именем \In{a} и затем вычисли \In{e}.
Выражение \In{case e of}~\verb!{alt1;...;alt2}! означает
узнай конструктор в корне \In{e} и продолжи вычисления в соответствующей
альтернативе. Обратите внимание на то, что сопоставления с образцом
в альтернативах имеет только один уровень вложенности. 
Также аргумент \In{case}-выражения в отличие от функции не обязан
быть атомарным. 

Для тренировки перепишем на STG пример из раздела про ленивые вычисления.

\begin{code}
data Nat = Zero | Succ Nat

zero    = Zero
one     = Succ zero
two     = Succ one

foldNat :: a -> (a -> a) -> Nat -> a
foldNat z  s  Zero      = z
foldNat z  s  (Succ n)  = s (foldNat z s n)

add a = foldNat a   Succ
mul a = foldNat one (add a) 

exp = (\x -> add (add x x) x) (add Zero two)
\end{code}

Теперь в STG:

\begin{code}
data Nat = Zero | Succ Nat

zero    = CON(Zero)
one     = CON(Succ zero)
two     = CON(Succ one)

foldNat = FUN( z s arg -> 
            case arg of 
                Zero    -> z
                Succ n  -> let next = THUNK (foldNat z s n)
                           in  s next  
          )

add     = FUN( a ->
            let succ = FUN( x -> 
                        let r = CON(Succ x) 
                        in r)
            in  foldNat a succ
          )

mul     = FUN( a -> 
            let succ = THUNK (add a)
            in  foldNat one succ
          )

exp     = THUNK( 
            let f = FUN( x -> let axx = THUNK (add x x)
                              in  add axx x) 
                a = THUNK (add Zero two)
            in  f a    
          )
\end{code}    

Программа состоит из связок вида \In{имя = объектКучи}.
Эти связки называют глобальными, они становятся
статическими объектами кучи, остальные объекты
выделяются динамически в \In{let}-выражениях.
Глобальный объект типа \In{THUNK} называют 
постоянной аппликативной формой (constant applicative form или 
сокращённо CAF).

\section{Вычисление STG}

Итак у нас есть упрощённый функциональный язык. Как мы будем
вычислять выражения? Присутствие частичного применения усложняет
этот процесс. Для многих функций мы не знаем заранее их арность.
Так например в выражении

\begin{code}
f x y
\end{code}

Функция \In{f} может иметь один аргумент в определении, 
но вернуть функцию. Есть два способа вычисления таких функций:

\begin{itemize}
\item \emph{вставка-вход} (push-enter). Когда мы видим 
применение функции, мы сначала \emph{вставляем} все аргументы в стек,
затем совершаем \emph{вход} в тело функции. В процессе входа
мы вычисляем функцию \In{f} и узнаём число аргументов, которое
ей нужно, после этого мы извлекаем из стека необходимое число 
аргументов, и применяем к ним функцию, если мы снова получаем функцию,
тогда мы опять добираем необходимое число аргументов из стека.
И так пока аргументы в стеке не кончатся.

\item \emph{вычисление-применение} (eval-apply). Вместе с функцией
мы храним информацию о том, сколько аргументов ей нужно. 
Если это статически определённая функция 
(определение выписано пользователем), то число аргументов мы можем
понять по левой части определения. В этой стратегии, если число
аргументов известно, мы сразу \emph{вычисляем} значение с
нужным числом аргументов, сохранив оставшиеся в стеке, 
а затем извлекаем аргументы из стека и \emph{применяем}
к ним вычисленное значение.
\end{itemize}

Возвращаясь к исходному примеру, предположим, что арность
функции \In{f} равна единице. Тогда стратегия вставка-вход
сначала добавит на стек \In{x} и \In{y}, а затем будет добирать
из стека необходимые аргументы. Стратегия вычисление-применение
сначала вычислит \In{(f x)}, сохранив \In{y} на стеке, затем
попробует применить результат к \In{y}. Почему мы говорим
попробует? Может так случиться, что арность значения \In{f x}
окажется равным трём, но пока у нас есть лишь один аргумент,
тогда мы создадим объект \In{PAP}, который соответствует 
частичному применению. 

Эти стратегии применимы как к ленивым, так и к энергичным языкам. 
Исторически сложилось, что ленивые языки тяготеют к первой 
стратегии, а энергичные ко второй. До недавнего времени
и в GHC применялась первая стратегия. Пока однажды разработчики GHC 
всё же не решили сравнить две стратегии. Реализовав обе стратегии,
и проверив их на большом количестве разных по сложности программ,
они пришли к выводу, что ни одна из стратегий не даёт существенного
преимущества на этапе вычислений. Потребление ресурсов оказалось 
примерно равным. Но вторая стратегия заметно выигрывала
в простоте реализации. Подробнее об этом можно почитать в статье
Simon Marlow, Simon Peyton Jones:
Making a Fast Curry: Push/Enter vs. Eval/Apply.
Описание модели вычислений GHC, которое вы сейчас читаете
копирует описание приведённое в этой статье. 

\subsection{Куча}

Объекты кучи принято называть \emph{замыканиями} (closure).
Их называют так, потому что обычно для вычисления выражения
нам не достаточно знать его текст, например посмотрим на
функцию:

\begin{code}
mul     = FUN( a -> 
            let succ = THUNK (add a)
            in  foldNat one succ
          )
\end{code}

Для того, чтобы вычислить \In{THUNK(add a)} нам необходимо
знать значение \In{a}, это значение определено в теле функции.
Оно определяется из контекста. По отношению к объекту
такую переменную называют \emph{свободной} (free). 
В куче мы будем хранить не только выражение \In{(add a)},
но и ссылки на все свободные переменные, которые участвуют
в выражении объекта. Эти ссылки называют \emph{довесок} (payload).
Объект кучи содержит ссылку на специальную таблицу и довесок. 
В таблице находятся информация о типе объекта и 
код, который необходимо вычислить, а также другая 
служебная информация. При вычислении объекта мы заменяем
ссылки настоящими значениями или ссылками на конструкторы. 

Объект кучи может быть:

\begin{itemize}
\item \In{FUN} -- определением функции;
\item \In{PAP} -- частичным применением;
\item \In{CON} -- полностью применённым конструктором;
\item \In{THUNK} -- отложенным вычислением;
\item \In{BLACKHOLE} -- это значение используется во время вычисления
    \In{THUNK}. Этот трюк предотвращает появление утечек памяти. 
\end{itemize}

Мы будем считать, что куча -- это таблица, которая ставит 
в соответствие адресам объекты или вычисленные значения.


\subsection{Стек}

Стек служит для быстрого переключения контекста. Мы будем пользоваться
стеком при вычислении \In{case}-выражений и \In{THUNK}-объектов. 
При вычислении \In{case}-выражения мы сохраняем в стеке альтернативы
и место возврата значения, а сами начинаем вычислять аргумент
\In{case}-выражения. При вычислении \In{THUNK}-объекта мы запомним
в стеке, адрес с которым необходимо связать полученное значение. 

При вычислении в стратегии вставка-вход мы будем сохранять 
в стеке аргументы функции. А при вычислении в стратегии 
вычисление-применение мы также будем сохранять аргументы 
функции в стеке. Какая разница между этими вариантами?
В первой стратегии мы можем доставать из стека произвольное
число аргументов, после определения арности функции мы добираем столько,
сколько нам нужно, поэтому мы будем хранить аргументы по одному.
Во второй же стратегии нам нужно просто сохранить все оставшиеся
аргументы. Мы сохраняем и извлекаем их все сразу. Упрощая,
объекты стека можно представить так:

\[\begin{array}{llll}
k   & ::=  & \texttt{case } \bullet \texttt{ of } \{ alt_1; \dots alt_n \} & 
        \text{контекст \texttt{case}-выражения}  \\
    & \Or  & Upd\ \ t\ \ \bullet                &  
        \text{Обновить отложенное вычисление} \\
    & \Or  & (\bullet \ a_1 \dots a_n )     &  
        \text{Применить функцию к аргументам, только для} \\
    &      &                             &  
        \text{стратегии вычисление-применение} \\
    & \Or  & Arg\ a                      &  
        \text{Аргумент на потом, только для}  \\
    &      &                             &  
        \text{стратегии вставка-вход} \\ 
\end{array}\]



\subsection{Правила общие для обеих стратегий вычисления}

Состояние вычислителя состоит из трёх частей. Это выражение
для вычисления $e$, стек $s$ и куча $H$. Мы рассмотрим правила
по которым вычислитель переходит из одного состояния в другое.
Все они имеют вид:

\[ e_1;\quad s_1;\quad H_1\quad \RA\quad e_2;\quad s_2;\quad H_2 \]

Левая часть переходит в правую, при условии, что левая часть
имеет определённый вид. Начнём с правил, которые одинаковы и в той
и в другой стратегии вычисления. Для простоты пока мы будем полагать,
что объекты только добавляются в кучу и никогда не стираются.
Мы будем обозначать добавление в стек как добавление элемента 
в обычный список: $elem\ :\ s$. 

Рассмотрим правило для \In{let}-выражений:

\[ \texttt{let } x \texttt{ = } obj \texttt{ in } e;\quad s;\quad H 
    \quad \RA \quad e[x'/x]; \quad s ;\quad H[x' \Ra obj],\quad x' 
    \text{ -- новое имя} \]

В этом правиле мы добавляем в кучу новый объект $obj$ 
под именем (или по адресу) $x'$. Запись $e[x'/x]$ означает
замену $x$ на $x'$ в выражении $e$. 

Теперь разберёмся с правилами для \In{case}-выражений. 

\[\begin{array}{lll}
\texttt{case}\ v\ \texttt{of}\ \{\dots;C\ x_1\dots x_n \Ra e ;\dots\};
    &\quad \RA  \quad &  e[a_1/x_1 \dots a_n / x_n]; \quad s; \quad H \\
s;\quad H[v \Ra CON(C\ a_1\dots a_n)] & & \\    
\\
\texttt{case}\ v\ \texttt{of}\ \{\dots; x \Ra e\};\quad s;\quad H 
    &\quad \RA  \quad & e[v/x];\quad s; \quad  H \\
    & & \text{Если } v \text{ -- литерал или } H[v] \text{ -- значение,} \\
    & & \text{которое не подходит ни по одной из альтернатив} \\
\\
\texttt{case}\ e\ \texttt{of}\ \{\dots\};\quad s; \quad H 
    & \quad \RA \quad & e;\quad 
    \texttt{case}\ \bullet \ \texttt{of}\ \{\dots\}\ :\ s; \quad H \\
\\

v;\quad \texttt{case}\ \bullet \ \texttt{of}\ \{\dots\}\ :\ s; \quad H 
    & \quad \RA \quad & \texttt{case}\ v\ \texttt{of}\ \{\dots\}; 
    \quad s; \quad H \\ 
\end{array}\]


Вычисления начинаются с третьего правила, в котором нам встречается
\In{case}-выражения с произвольным $e$. В этом правиле мы сохраняем
в стеке альтернативы и адрес возвращаемого значения и продолжаем
вычисление выражения $e$. После вычисления мы перейдём к четвёртому правилу,
тогда мы снимем со стека информацию необходимую для продолжения вычисления
\In{case}-выражения. Это приведёт нас к одному из первых двух правил.
В первом правиле значение аргумента содержит конструктор, подходящий
по одной из альтернатив, а во втором мы выбираем альтернативу по умолчанию.

Теперь посмотрим как вычисляются \In{THUNK}-объекты.

\[\begin{array}{lll}
x;\quad s; \quad H[x \Ra THUNK\ e]  &\quad \RA  \quad & 
    e; \quad Upd\ \ x\ \ \bullet\ :\ s; \quad H[x \Ra BLACKHOLE] \\
\\

y;\quad Upd\ \ x\ \ \bullet\ :\ s; \quad H
&\quad \RA  \quad & y; \quad s; \quad H[x \Ra H[y]] \\ 
& & \text{если } H[y] \text{ является значением} \\
\end{array}\]

Если переменная указывает на отложенное вычисление $e$, 
мы сохраняем в стеке адрес по которому необходимо обновить
значение и вычисляем значение $e$. В это время мы записываем
в по адресу $x$ объект $BLACKHOLE$. У нас нет такого правила,
которое реагирует на левую часть, если в ней содержится
объект $BLACKHOLE$. Поэтому во время вычисления $THUNK$ 
ни одно из правил сработать не может. Этот трюк необходим
для избежания утечек памяти. Как только выражнение будет
вычислено мы извлечём из стека адрес $x$ и обновим значение.

Правила применения функций, если арность совпадает
с числом аргументов в тексте выражения:

\[\begin{array}{lll}
f^n\ a_1 \dots a_n; \quad s; \quad H[y \Ra FUN(x_1 \dots x_n \Ra e)] 
&\quad \RA  \quad &  e[a_1/x_1 \dots a_n/x_n]; s; H \\
\\
\oplus\ a_1 \dots a_n; \quad s; \quad H
&\quad \RA  \quad & a;\quad s;\quad H \\
& & a \text{ -- результат вычисления } (\oplus\ a_1 \dots a_n)  \\
\end{array}\]

Мы просто заменяем все вхождения аргументов на значения. Второе
правило выполняет применение примитивной функции к значениям.

\subsection{Правила для стратегии вставка-вход}

\[\begin{array}{lll}
f^k\ a_1 \dots a_m ; \quad s; \quad H 
&\quad \RA  \quad &  f ;\quad Arg\ a_1: \text{\dots} : \ Arg\ a_m \ :\ s ;\quad H \\
\\
\multicolumn{3}{l}{f; \quad Arg\ a_1: \text{\dots} : \ Arg\ a_n \ :\ s ;\quad 
    H[f \Ra FUN(x_1 \dots x_n \Ra e)]} \\ 
&\quad \RA  \quad & e[a_1/x_1 \dots a_n/x_n] ;\quad s; \quad H \\
\\
\multicolumn{3}{l}{f; \quad Arg\ a_1: \text{\dots} : \ Arg\ a_m \ :\ s ;\quad 
    H[f \Ra FUN(x_1 \dots x_n \Ra e)]} \\ 
&\quad \RA  \quad & p;\quad s; \quad H[p \Ra PAP(f\ a_1 \dots a_m)] \\
&  & \text{при } m \geq 1;\ m < n;\ \text{верхний элемент } s \\
&  & \text{ не является } Arg;\ p\ \text{-- новый адрес} \\
\\
\multicolumn{3}{l}{f; \quad Arg\ a_{n+1}\ :\ s;
    \quad H[f \Ra PAP(g\ a_1 \dots a_n)]} \\
&\quad \RA  \quad & g;\quad 
    Arg\ a_1: \text{\dots} : \ Arg\ a_n \ : \ Arg\ a_{n+1} \ :\ s ;\quad H
\end{array}\]

Первое правило выполняет этап \Quote{вставка}. 
Если мы видим применение функции,
мы первым делом сохраняем все аргументы в стеке. Во втором
правиле мы вычислили значение \In{f}, оно оказалось
функцией с арностью $n$. Тогда мы добираем из стека $n$ 
аргументов и подставляем их в правую часть функции $e$.
Если  в стеке оказалось слишком мало аргументов,
то мы переходим к третьему правилу и составляем 
частичное применение. Последнее правило говорит о том как
расшифровывается частичное применение. Мы вставляем в стек
все аргументы и начинаем вычисление функции $g$ из тела $PAP$.



\subsection{Правила для стратегии вычисление-применение}

\[\begin{array}{lll}
\multicolumn{3}{l}{f^{\bullet}\ a_1 \dots a_n;\quad s;\quad 
    H[f \Ra FUN(x_1 \dots x_n \Ra e)]} \\
    & \quad \RA  \quad &  e[a_1/x_1 \dots a_n/x_n];\quad s;\quad H \\
\\
\multicolumn{3}{l}{f^k\ a_1 \dots a_m;\quad s;\quad 
    H[f \Ra FUN(x_1 \dots x_n \Ra e)]} \\
    & \quad \RA  \quad &  e[a_1/x_1 \dots a_n/x_n];\quad 
        (\bullet \ a_{n+1} \dots a_m)\ :\ s;\quad H \\ 
    & &  \text{при } m \geq n \\
    & \quad \RA  \quad & p;\quad s ;\quad H[p \Ra PAP(f\ a_1 \dots a_m)] \\
    & &  \text{при } m < n,\ p \text{ -- новый адрес} \\
    \\
\multicolumn{3}{l}{f^{\bullet}\ a_1 \dots a_m;\quad s;\quad 
    H[f \Ra THUNK\ e]} \\
    & \quad \RA  \quad &  f;\quad (\bullet \ a_1 \dots a_m)\ :\ s;\quad H \\ 
\\
\multicolumn{3}{l}{f^k\ a_{n+1} \dots a_m;\quad s;\quad 
    H[f \Ra PAP(g\ a_1 \dots a_n)]} \\
    & \quad \RA  \quad &  g^\bullet\ a_1 \dots a_n\ a_{n+1} \dots a_m; 
        \quad s; \quad H \\ 
\\
f;\quad (\bullet \ a_1 \dots a_n)\ :\ s;\quad H 
    & \quad \RA  \quad & f^\bullet\ a_1 \dots a_n;\quad s;\quad H \\ 
    & & H[f] \text{ является } FUN \text{ или } PAP
\\
\end{array}\]

Разберёмся с первыми двумя правилами.
В первом правиле статическая арность $f$ неизвестна, но
значение $f$ уже вычислено, и мы можем узнать арность по
объекту $FUN$, далее возможны три случая. Число аргументов
переданных в функцию совпадает с арностью $FUN$, тогда
мы применяем аргументы к правой части $FUN$. Если
в функцию передано больше аргументов чем нужно, мы 
сохраняем лишние на стеке. Если же аргументов меньше,
то мы создаём объект $PAP$. 
Третье правило говорит о том, что нам делать, если значение
$f$ ещё не вычислено. Оно является $THUNK$. Тогда мы 
сохраним аргументы на стеке и вычислим его. В следующем
правиле мы раскрываем частичное применение. Мы просто
организуем вызов функции со всеми аргументами (и со стека
и из частичного применения). Последнее правило срабатывает
после третьего. Когда мы вычислим $THUNK$ и увидим там 
$FUN$ или $PAP$. Тогда мы составляем применение функции.  

Сложность применения стратегии вставка-вход связана
с плохо предсказуемым изменением стека. Если в стратегии вычисление-выполнение
мы добавляем и снимаем все аргументы, то в стратегии вставка-вход
мы добавляем их по одному и неизвестно сколько снимем в следующий раз.
Кроме того стратегия вычисление-применение позволяет проводить
оптимизацию перемещения аргументов. Вместо стека мы можем хранить
аргументы в регистрах. Тогда скорость обращения к аргументам
резко возрастёт. 

\section{Управление памятью. Сборщик мусора}

В прошлом разделе для простоты мы считали, что объекты только добавляются
в кучу. На самом деле это не так. Допустим во время вычисления функции
нам нужно было вычислить какие-то промежуточные данные, например
объявленные в локальных переменных, тогда после вычисления результата
все эти значения больше не нужны. При этом в куче висит
много-много объектов, которые уже не нужны. Нам нужно 
как-то от них избавится. Этой задачей занимается отдельный 
блок вычислителя, который называется \emph{сборщиком мусора} 
(garbage collector), соответственно процесс автоматического
освобождения памяти называется сборкой мусора (garbage collection или GC).

На данный момент в GHC используется копирующий последовательный
сборщик мусора, который работает по алгоритму Чейни (Cheney). 
Для начала рассмотрим простой алгоритм сборки мусора.
Мы выделяем небольшой объём памяти и начинаем наполнять
его объектами. Как только место кончится мы 
найдём все \Quote{живые} объекты, а остальное
пространство памяти будем считать свободным. 
Как только после очередной очистки оказалось, что 
нам всё же не хватает места. Мы найдём все живые 
объекты, подсчитаем сколько места они занимают и
запросим у системы этот объём памяти. Скопируем все
живые объекты на новое место, а старую память будем 
считать свободной. 
Так например, если у нас было выделено 30 Мб памяти и оказалось,
что живые объекты занимают 10 Мб, мы выделим ещё 10 Мб, скопируем
туда все живые объекты и общий объём памяти станет равным 40 Мб.

Мы можем оптимизировать сборку мусора. Есть такая гипотеза,
что большинство объектов имеют очень короткую жизнь.
Это промежуточные данные, локальные переменные. 
Нам нужен лишь результат функции, но на подходе к 
результату мы сгенерируем много разовой информации.
Ускорить очистку можно так. Мы выделим совсем небольшой
участок памяти внутри нашей кучи, его принято
называть \emph{яслями} (nursery area), и будем выделять и собирать
новые объекты только в нём, как только этот участок 
заполнится мы скопируем все живые объекты из яслей
в остальную память и снова будем наполнять ясли. 
Как только вся память закончится мы поступим
так же как и в предыдущем сценарии. 
Когда заканчивается место в яслях, мы проводим 
поверхностную очистку (minor GC), а когда заканчивается
вся память в текущей куче, мы проводим глубокую очистку
(major GC). Эта схема соответствует сборке с двумя
поколениями. 

\section{Статистика выполнения программы}

Процесс управления памятью скрыт от программиста. 
Но при этом в GHC есть развитые средства косвенной
диагностики работы программы. Пока мы пользовались
самым простым способом проверки. Мы включали флаг \In{s} в 
интерпретаторе. Пришло время познакомиться и с другими.

\subsection{Статистика вычислителя}

Для начала научимся смотреть статистику работы вычислителя.
Посмотреть статистику можно с помощью флагов 
\In{s[file]} и \In{S[file]}. Эти флаги предназначены 
для вычислителя низкого уровня 
(realtime system или RTS, далее просто вычислитель), 
они заключаются в окружение \In{+RTS ... -RTS}, если флаги 
идут в конце строки и считается,
что все последующие флаги предназначены для \In{RTS}
мы можем просто написать \In{ghc --make file.hs +RTS ...}
Например скомпилируем такую программу:
 
\begin{code}
module Main where

main = print $ sum [1 .. 1e5]
\end{code}

Теперь скомпилируем:

\begin{code}
$ ghc --make sum.hs -rtsopts -fforce-recomp
\end{code}

Флаг \In{rtsopts} позволяет передавать скомпилированной программе
флаги для вычислителя низкого уровня, далее для краткости мы будем называть
его просто вычислителем. С флагом \In{fforce-recomp} программа будет
каждый раз заново пересобираться. Теперь посмотрим на статистику 
выполнения программы (флаг \In{s[file]}, в этом примере мы перенаправляем
выход в поток \In{stderr}):

\begin{code}
$ ./sum +RTS -sstderr
5.00005e9
      14,145,284 bytes allocated in the heap
      11,110,432 bytes copied during GC
       2,865,704 bytes maximum residency (3 sample(s))
         460,248 bytes maximum slop
               7 MB total memory in use (0 MB lost due to fragmentation)

                                    Tot time (elapsed)  Avg pause  Max pause
  Gen  0        21 colls,     0 par    0.00s    0.01s     0.0006s    0.0036s
  Gen  1         3 colls,     0 par    0.01s    0.01s     0.0026s    0.0051s

  INIT    time    0.00s  (  0.00s elapsed)
  MUT     time    0.01s  (  0.01s elapsed)
  GC      time    0.01s  (  0.02s elapsed)
  EXIT    time    0.00s  (  0.00s elapsed)
  Total   time    0.02s  (  0.03s elapsed)

  %GC     time      60.0%  (69.5% elapsed)

  Alloc rate    1,767,939,507 bytes per MUT second

  Productivity  40.0% of total user, 26.0% of total elapsed
\end{code}

Был распечатан результат и отчёт о работе программы. Разберёмся
с показателями:

\begin{code}
bytes allocated in the heap  -- число байтов выделенных в куче
                             -- за всё время работы программы
bytes copied during GC       -- число скопированных байтов
                             -- за всё время работы программы
bytes maximum residency      -- в каком объёме памяти работала программа 
                             -- в скобках указано число глубоких очисток
bytes maximum slop           -- максимум потерь памяти из-за фрагментации

total memory in use          -- сколько всего памяти было запрошено у ОС
\end{code}

Показатель \In{bytes maximum residency} измеряется только
при глубокой очистке, поскольку новая память выделяется именно
в этот момент. Размер памяти выделенной в куче гораздо больше
общего объёма памяти. Так происходит потому, что этот показатель
указывает на общее число памяти в куче за всё время работы программы.
Ведь мы переиспользуем не нужную нам память. По этому
показателю можно судить о том, сколько замыканий (объектов) 
было выделено в куче. 

Следующие две строчки говорят о числе сборок мусора. Мы видим,
что GC выполнил 21 поверхностную очистку (поколение 0) и 
3 глубоких (покколение 1). 
Дальше идут показатели скорости. \In{INIT} и \In{EXIT} -- это 
инициализация и завершение программы. \In{MUT} -- это полезная
нагрузка, время, которая наша программа тратила на изменение (MUTation)
значений. \In{GC} -- время сборки мусора. Далее GHC сообщил
нам о том, что мы провели 60\% времени в сборке мусора.
Это очень плохо. Продуктивность программы очень низкая. 
Затратна глубокая сборка мусора, поверхностная -- это дело обычное.
Теперь посмотрим на показатели строгой версии этой программы:

\begin{code}
module Main where

import Data.List(foldl')

sum' = foldl' (+) 0

main = print $ sum' [1 .. 1e5]
\end{code}

Скомпилируем:

\begin{code}
$ ghc --make sumStrict.hs -rtsopts -fforce-recomp
\end{code}

Посмотрим на статистику:

\begin{code}
$ ./sumStrict +RTS -sstderr
5.00005e9
      10,474,128 bytes allocated in the heap
          24,324 bytes copied during GC
          27,072 bytes maximum residency (1 sample(s))
          27,388 bytes maximum slop
               1 MB total memory in use (0 MB lost due to fragmentation)

                                    Tot time (elapsed)  Avg pause  Max pause
  Gen  0        19 colls,     0 par    0.00s    0.00s     0.0000s    0.0000s
  Gen  1         1 colls,     0 par    0.00s    0.00s     0.0001s    0.0001s

  INIT    time    0.00s  (  0.00s elapsed)
  MUT     time    0.01s  (  0.01s elapsed)
  GC      time    0.00s  (  0.00s elapsed)
  EXIT    time    0.00s  (  0.00s elapsed)
  Total   time    0.01s  (  0.01s elapsed)

  %GC     time       0.0%  (3.0% elapsed)

  Alloc rate    1,309,266,000 bytes per MUT second

  Productivity 100.0% of total user, 116.0% of total elapsed
\end{code}

Мы видим, что произошла лишь одна глубокая сборка. И это
существенно сказалось на продуктивности. Кромке того мы видим,
что программа заняла лишь 27 Кб памяти, вместо 2 Мб как в прошлом
случае. Теперь давайте покрутим ручки у GC. 
В GHC можно устанавливать разные параметры сборки
мусора с помощью флагов. Все флаги можно посмотреть в 
документации GHC. Мы обратим
внимание на несколько флагов. Флаг \In{H} назначает
минимальное значение для стартового объёма кучи. 
Флаг \In{A} назначает объём памяти для яслей. 
По умолчанию размер яслей равен 512 Кб 
(эта цифра может измениться). Изменением этих
параметров мы можем отдалить сборку мусора. 
Чем дольше работает программа между сборками,
тем выше вероятность того, что многие объекты 
уже не нужны. 

Давайте убедимся в том, что поверхностные очистки происходят
очень быстро и совсем не тормозят программу. Установим размер яслей
на 32 Кб вместо 512 Кб как по умолчанию (размер пишется сразу за флагом,
за цифрой идёт k или m):

\begin{code}
$ ./sumStrict +RTS -A32k -sstderr
...
                                    Tot time (elapsed)  Avg pause  Max pause
  Gen  0       318 colls,     0 par    0.00s    0.00s     0.0000s    0.0000s
  Gen  1         1 colls,     0 par    0.00s    0.00s     0.0001s    0.0001s
...
  MUT     time    0.01s  (  0.01s elapsed)
  GC      time    0.00s  (  0.00s elapsed)
...
  %GC     time       0.0%  (11.8% elapsed)
\end{code}

Мы видим, что за счёт уменьшения памяти очистки существенно участились, 
но это не сказалось на общем результате. С помощью флага \In{H[size]}
мы можем устанавливать рекомендуемое минимальное значение для размера
кучи. Оно точно не будет меньше. Вернёмся к первому варианту 
и выделим алгоритму побольше памяти, например 20 Мб:

\begin{code}
./sum +RTS -A1m -H20m -sstderr
5.00005e9
      14,145,284 bytes allocated in the heap
         319,716 bytes copied during GC
         324,136 bytes maximum residency (1 sample(s))
          60,888 bytes maximum slop
              22 MB total memory in use (1 MB lost due to fragmentation)

                                    Tot time (elapsed)  Avg pause  Max pause
  Gen  0         2 colls,     0 par    0.00s    0.00s     0.0001s    0.0001s
  Gen  1         1 colls,     0 par    0.00s    0.00s     0.0007s    0.0007s

  INIT    time    0.00s  (  0.00s elapsed)
  MUT     time    0.02s  (  0.02s elapsed)
  GC      time    0.00s  (  0.00s elapsed)
  EXIT    time    0.00s  (  0.00s elapsed)
  Total   time    0.02s  (  0.02s elapsed)

  %GC     time       0.0%  (4.4% elapsed)

  Alloc rate    884,024,998 bytes per MUT second

  Productivity 100.0% of total user, 78.6% of total elapsed
\end{code}

Произошла лишь одна глубокая очистка (похоже, что 
эта очистка соответствует начальному выделению памяти) и 
продуктивность программы стала стопроцентной. С помощью 
флага \In{S} вместо \In{s} мы можем посмотреть более 
детальную картину управления памяти. Будут распечатаны 
показатели памяти для каждой очистки. 

\begin{code}
./sum +RTS -Sfile
\end{code}

В файле \In{file} мы найдём такую таблицу:

\begin{code}
        память                          время
выделено скопировано в живых    GC             Total             Тип очистки

 Alloc    Copied     Live    GC    GC     TOT     TOT  Page Flts
 bytes     bytes     bytes  user  elap    user    elap
545028    150088    174632  0.00  0.00    0.00    0.00    0    0  (Gen:  1)
523264    298956    324136  0.00  0.00    0.00    0.00    0    0  (Gen:  0)
...
\end{code}

Итак у нас появился один существенный показатель качества
программ. Это количество глубоких очисток. Во время глубокой
очистки вычислитель производит две затратные операции:
сканирование всей кучи и запрос у системы возможно большого блока памяти. 
Чем меньше таких очисток, тем лучше. Сократить их число
можно удачной комбинацией показателей \In{A} и \In{H}. 
Но не стоит сразу начинать обновлять параметры по умолчанию,
если ваша программа работает слишком медленно. Лучше 
сначала попробовать изменить алгоритм. Найти
функцию, которая слишком много ленится и ограничить её с помощью 
\In{seq} или энергичных образцов. 
В этом примере у нас
была всего одна функция, поэтому поиск не составил труда.
Но что если их уже очень много? Скорее всего так и будет.
Не стоит оптимизировать не рабочую программу. 
А в рабочей программе обычно много функций.
Но это не так страшно, помимо суммарных показателей
GHC позволяет собирать более конкретную статистику. 

Стоит отметить 
функцию \In{performGC} из модуля \In{System.Mem},
она форсирует поверхностную сборку мусора. 
Допустим вы чистаете какие-то данные из файла и тут 
же преобразуете их в структуру данных. После того
как чтение данных закончится, вы знаете, что промежуточные
данные связаные с чтением вам уже не нужны. Выполнив
\In{performGC} вы можете подсказать об этом вычислителю.

\subsection{Профилирование функций}

\subsubsection{Время и общий объём памяти}

Процесс отслеживания показателей память/скорость 
называется профилированием программы. Всё вроде бы работает,
но работает слишком медленно, необходимо установить причину.
Рассмотрим такую программу:

\begin{code}
module Main where

concatR = foldr (++) [] 
concatL = foldl (++) []

fun :: Double
fun = test concatL - test concatR
    where test f = last $ f $ map return [1 .. 1e6]

main = print fun
\end{code}

У нас есть подозрение, что какая-то из двух функций 
\In{concatX} работает слишком медленно.  Мы можем
посмотреть какая, если добавим к ним специальную прагму \In{SCC}:

\begin{code}
concatR = {-# SCC "right" #-} foldr (++) [] 
concatL = {-# SCC "left"  #-} foldl (++) []
\end{code}

Напомню, что прагмой называется специальный блочный комментарий
с решёткой. Это специальное сообщение компилятору. Прагмой
\In{SCC} мы устанавливаем так называемый центр затрат (cost center).
Она пишется сразу за знаком равно. В кавычках пишется имя, под
которым статиситика войдёт в итоговый отчёт. 
После этого вычислитель будет следить
за нагрузкой, которая приходится на эту функцию.
Теперь нам нужно скомпилировать модуль с флагом \In{prof},
который активирует подсчёт статистики в центрах затрат:

\begin{code}
$ ghc --make concat.hs -rtsopts -prof -fforce-recomp
$ ./concat +RTS -p
\end{code}
  
Второй командой мы запускаем программу и передаём вычислителю флаг \In{p}.
После этого будет создан файл \In{concat.prof}. Откроем этот файл:

\begin{code}
	   concat +RTS -p -RTS

	total time  =        1.45 secs   (1454 ticks @ 1000 us, 1 processor)
	total alloc = 1,403,506,324 bytes  (excludes profiling overheads)

COST CENTRE MODULE  %time %alloc

left        Main     99.8   99.8


                                                                 individual     inherited
COST CENTRE MODULE                             no.   entries  %time %alloc   %time %alloc

MAIN        MAIN                                46         0    0.0    0.0   100.0  100.0
 CAF        GHC.Integer.Logarithms.Internals    91         0    0.0    0.0     0.0    0.0
 CAF        GHC.IO.Encoding.Iconv               71         0    0.0    0.0     0.0    0.0
 CAF        GHC.IO.Encoding                     70         0    0.0    0.0     0.0    0.0
 CAF        GHC.IO.Handle.FD                    57         0    0.0    0.0     0.0    0.0
 CAF        GHC.Conc.Signal                     56         0    0.0    0.0     0.0    0.0
 CAF        Main                                53         0    0.2    0.2   100.0  100.0
  right     Main                                93         1    0.0    0.0     0.0    0.0
  left      Main                                92         1   99.8   99.8    99.8   99.8
\end{code}
  
Мы видим, что почти всё время работы программа провела в функции
\In{concatL}. Функция \In{concatR} была вычислена мгновенно (\In{time}) и
почти не потребовала ресусов памяти (\In{alloc}). У нас есть две пары 
колонок результатов. \In{individual} указывает на время вычисления функции,
а \In{inherited}~-- на время вычисления функции и всех дочерних функций.
Колонка \In{entries} указывает число вызовов функции. 
Если мы хотим проверить все функции мы можем не указывать функции
прагмами. Для этого при компиляции указывается флаг \In{auto-all}.
Отметим также, что все константы определённый на самом верхнем уровне
модуля, сливаются в один центр. Они называются в отчёте как \In{CAF}.
Для того чтобы вычислитель следил за каждой константой по отдельности
необходимо указать флаг \In{caf-all}. Попробуем на таком модуле:

\begin{code}
module Main where

fun1 = test concatL - test concatR
fun2 = test concatL + test concatR

test f = last $ f $ map return [1 .. 1e4]

concatR = foldr (++) [] 
concatL = foldl (++) [] 

main = print fun1 >> print fun2
\end{code}
  
Скомпилируем:

\begin{code}
$ ghc --make concat2.hs -rtsopts -prof -auto-all -caf-all -fforce-recomp
$ ./concat2 +RTS -p
0.0
20000.0
\end{code}
  
После этого можно открыть файл \In{concat2.prof} и посмотреть 
итоговую статистику по всем значениям. Программа с включённым
профилированием будет работать гораздо медленей, не исключено,
что ей не хватит памяти на стеке, в этом случае вы можете добавить
памяти с помощью флага вычислителя \In{K}, впрочем если это произойдёт
GHC подскажет вам что делать. 

\subsubsection{Динамика изменения объёма кучи}

В предыдущем разделе мы смотрели общее время и память
затраченные на вычисление функции. В этом мы научимся
измерять динамику изменения расхода памяти на куче. 
По этому показателю можно понять в какой момент 
в программе возникают утечки памяти. Мы увидим характерные
горбы на картинках, когда GC будет активно запрашивать новую память.
Для этого сначала нужно скомпилировать программу с флагом \In{prof}
как и в предыдущем разделе, а при выполнении программы 
добавить один из флагов \In{hc}, \In{hm}, \In{hd}, \In{hy} или \In{hr}.
Все они начинаются с буквы \In{h}, от слова \In{heap} (куча). 
Вторая буква указывает тип графика, какими показателями
мы интересуемся. Все они создают специальный файл 
\In{имяПриложения.hp}, который мы можем преобразовать в
график в формате \In{PostScript} с помощью программы
\In{hp2ps}, она устанавливается автоматически вместе с GHC.

Рассмотрим типичную утечку памяти (из упражнения к предыдущей главе):


\begin{code}
module Main where

import System.Environment(getArgs)

main = print . sum2 . xs . read =<< fmap head getArgs  
    where xs n = [1 .. 10 ^ n]

sum2 :: [Int] -> (Int, Int)
sum2 = iter (0, 0)
    where iter c  []     = c
          iter c  (x:xs) = iter (tick x c) xs

tick :: Int -> (Int, Int) -> (Int, Int)
tick x (c0, c1) | even x    = (c0, c1 + 1)
                | otherwise = (c0 + 1, c1)
\end{code}

Скомпилируем с флагом профилирования:

\begin{code}
$ ghc --make leak.hs -rtsopts -prof -auto-all
\end{code}

Статистика вычислителя показывает, что эта программа
вызывала глубокую очистку 8 раз и выполняла полезную работу
лишь 40\% времени.

\begin{code}
$ ./leak 6 +RTS -K30m -sstderr
...
                                    Tot time (elapsed)  Avg pause  Max pause
  Gen  0       493 colls,     0 par    0.26s    0.26s     0.0005s    0.0389s
  Gen  1         8 colls,     0 par    0.14s    0.20s     0.0248s    0.0836s
...
    Productivity  40.5% of total user, 35.6% of total elapsed

\end{code}
  
Теперь посмотрим на профиль кучи.

\begin{code}
$ ./leak 6 +RTS -K30m -hc
(500000,500000)
$ hp2ps -e80mm -c leak.hp
\end{code}

В первой команде мы добавили флаг \In{hc} для того, чтобы создать
файл с расширением \In{.hp}. Он содержит таблицу с показателями
размера кучи, которые вычислитель замеряет через равные промежутки
времени. Мы можем изменять интервал с помощью флага \In{iN},
где \In{N}~-- время в секундах. Второй командой мы преобразуем
профиль в картинку. Флаг \In{c}, говорит о том, что мы хотим 
получить цветную картинку, а флаг \In{e80mm}, говорит о том,
что мы собираемся вставить картинку в текст LaTeX. После \In{e}
указан размер в миллиметрах. Мы видим характерный горб \RefFig{leak}.

\Fig{Профиль кучи для утечки памяти}{../pic/10/leak.ps}{leak}{1.6}

В картинку не поместились имена функций мы можем увеличить 
строку флагом \In{L}. Теперь все имена поместились (\RefFig{leak_1}).

\begin{code}
$ ./leak 6 +RTS -K30m -hc -L45
(500000,500000)
$ hp2ps -e80mm -c leak.hp
\end{code}

\Fig{Профиль кучи для утечки памяти}{../pic/10/leak_1.ps}{leak_1}{1.6}

С помощью флага \In{hd} посмотрим на объекты, которые 
застряли в куче (\RefFig{leak_2}):

\begin{code}
$ ./leak 6 +RTS -K30m -hd -L45
(500000,500000)
$ hp2ps -e80mm -c leak.hp
\end{code}

На \RefFig{leak_2} куча разбита по типу объектов (замыканий). \In{BLACKHOLE}
это специальный объект, который заменяет \In{THUNK}
во время его вычисления. \In{I}\verb!#! -- это скрытый конструктор
\In{Int}. \In{sat_sUa} и \In{sat_sUd} -- это имена застрявших
отложенных вычислений. Если бы наша программа была очень большой
на этом месте мы бы запустили профилирование по функциям
с флагом \In{p} и из файла \In{leak.prof} узнали бы в каких
функциях программа тратит больше всего ресурсов.
После этого мы бы пошли смотреть исходный код подозрительных
функций и после внесённых изменений снова посмотрели бы 
на графики кучи. 


\Fig{Профиль кучи для утечки памяти}{../pic/10/leak_2.ps}{leak_2}{1.6}


Если подумать, что мы делаем? Мы создаём отложенное вычисление,
которое обещает построить большой список, вытягиваем из списка 
по одному элементу и, если элемент оказывается чётным, прибавляем
к одному элементу пары, а если не чётным, то к другому. 
Проблема в том, что внутри пары происходит накопление отложенных
вычислений, необходимо сразу вычислять значения перед запаковыванием
их в пару. Изменим код:

\begin{code}
{-# Language BangPatterns #-}
module Main where

import System.Environment(getArgs)

main = print . sum2 . xs . read =<< fmap head getArgs  
    where xs n = [1 .. 10 ^ n]

sum2 :: [Int] -> (Int, Int)
sum2 = iter (0, 0)
    where iter c  []     = c
          iter c  (x:xs) = iter (tick x c) xs

tick :: Int -> (Int, Int) -> (Int, Int)
tick x (!c0, !c1) | even x    = (c0, c1 + 1)
                  | otherwise = (c0 + 1, c1)
\end{code}
  
Мы сделали функцию \In{tick} строгой. Теперь посмотрим на профиль:

\begin{code}
$ ghc --make leak2.hs -rtsopts -prof -auto-all
$ ./leak2 6 +RTS -K30m -hc
(500000,500000)
$ hp2ps -e80mm -c leak2.hp
\end{code}
  
Не получилось (\RefFig{leak2}). Как же так. Посмотрим 
на расход памяти отдельных функций.
\In{tick} стала строгой, но этого не достаточно, потому что
в первом аргументе \In{iter}  накапливаются вызовы \In{tick}.
Сделаем \In{iter} строгой по первому аргументу:

\Fig{Опять двойка}{../pic/10/leak2.ps}{leak2}{1.6}


\begin{code}
sum2 :: [Int] -> (Int, Int)
sum2 = iter (0, 0)
    where iter !c  []     = c
          iter !c  (x:xs) = iter (tick x c) xs
\end{code}

Теперь снова посмотрим на профиль:

\begin{code}
$ ghc --make leak2.hs -rtsopts -prof -auto-all
$ ./leak2 6 +RTS -K30m -hc
(500000,500000)
$ hp2ps -e80mm -c leak2.hp
\end{code}
  

Мы видим (\RefFig{leak3}), что память резко подскакивает
и остаётся постоянной. Но теперь показатели измеряются не в мегабайтах,
а в килобайтах. Мы справились. Остальные флаги \In{hX}
позволяют наблюдать за разными специфическими объектами
в куче. Мы можем узнать сколько памяти приходится на
разные модули (\In{hm}), сколько памяти приходится на 
разные конструкторы (\In{hd}), на разные типы замыканий (\In{hy}).

\Fig{Профиль кучи без утечки памяти}{../pic/10/leak3.ps}{leak3}{1.6}





\subsection{Поиск источников внезапной остановки}

\In{case}-выражения и декомпозиция в аргументах функции
могут стать источником очень неприятных ошибок. Программа
прошла проверку типов, завелась и вот уже работает-работает
как вдруг мы видим на экране:

\begin{code}
*** Exception: Prelude.head: empty list
\end{code}

или 

\begin{code}
*** Exception: Maybe.fromJust: Nothing
\end{code}

И совсем не понятно откуда эта ошибка. В каком модуле
сидит эта функция. Может мы её импортировали из чужой библиотеки
или написали сами. Как раз для таких случаев в GHC предусмотрен
специальный флаг \In{xc}. 

Посмотрим на выполнение такой программы:

\begin{code}
module Main where

addEvens :: Int -> Int -> Int
addEvens a b 
    | even a && even b = a + b

q = zipWith addEvens [0, 2, 4, 6, 7, 8, 10] (repeat 0)

main = print q
\end{code}
  
Для того, чтобы воспользоваться флагом \In{xc} необходимо
скомпилировать программу с возможностью профилирования:

\begin{code}
$ ghc --make break.hs -rtsopts -prof
$ ./break +RTS -xc
*** Exception (reporting due to +RTS -xc): (THUNK_2_0), stack trace: 
  Main.CAF
break: break.hs:(4,1)-(5,30): Non-exhaustive patterns in function addEvens
\end{code}
  
Так мы узнали в каком месте кода проявился злосчастный вызов, это
строки \In{(4,1)-(5,30)}. Что соответствует определению 
функции \In{addEvens}. Не очень полезная информация. Мы и так
бы это узнали. Нам бы хотелось узнать тот путь, по которому
шла программа к этому вызову. Проблема в том, что все вызовы
слились в один \In{CAF} для модуля. Так разделим их:

\begin{code}
$ ghc --make break.hs -rtsopts -prof -caf-all -auto-all
$ ./break +RTS -xc
*** Exception (reporting due to +RTS -xc): (THUNK_2_0), stack trace: 
  Main.addEvens,
  called from Main.q,
  called from Main.CAF:q
  --> evaluated by: Main.main,
  called from :Main.CAF:main
break: break.hs:(4,1)-(5,30): Non-exhaustive patterns in function addEvens
\end{code}
  
Теперь мы видим путь к этому вызову, мы пришли в него из 
знчения \In{q}, которое было вызвано из \In{main}.

\section{Оптимизация программ}

В этом разделе мы поговорим о том этапе компиляции, на
котором происходят преобразования \In{Core -> Core}. 
Мы называли этот этап упрощением программы.

\subsection{Флаги оптимизации}

Мы можем задавать степень оптимизации программы 
специальными флагами. Самые простые флаги начинаются
с большой буквы \In{O}. Естесственно, чем больше мы оптимизируем,
тем дольше компилируется код. Поэтому не стоит увлекаться 
оптимизацией на начальном этапе проектирования.
Посмотрим какие возможности у нас есть:

\begin{itemize}
\item без \In{-O} -- минимум оптимизаций, код компилируется как можно быстрее.
\item \In{-O0} -- выключить оптимизацию полностью
\item \In{-O} -- умеренная оптимизация.
\item \In{O2} -- активная оптимизация, код компилируется дольше, но
    пока \In{O2} не сильно выигрывает у \In{O} по продуктивности. 
\end{itemize}

Для оптимизации мы компилируем программу с заданным флагом,
например попробуйте скомпилировать самый первый пример
с флагом \In{O}:

\begin{code}
ghc --make sum.hs -O 
\end{code}
  
\noindent и утечка памяти исчезнет.

Посмотреть описание конкретных шагов оптимизации можно
в документации к GHC. Например при включённой оптимизации 
GHC применяет анализ строгости. В ходе него GHC может
исправить простые утечки памяти за нас. Стоит отметить
оптимизацию \In{-fexcess-precision}, он может существенно
ускорить программы, в которых много вычислений с \In{Double}.
Но при этом вычисления могут потерять в точности, 
округление становится непредсказуемым.

\subsection{Прагма INLINE}

Если мы посмотрим в исходный файл для модуля \In{Prelude},
то мы найдём такое определение для композиции функций:

\begin{code}
-- | Function composition.
{-# INLINE (.) #-}
-- Make sure it has TWO args only on the left, so that it inlines
-- when applied to two functions, even if there is no final argument
(.)    :: (b -> c) -> (a -> b) -> a -> c
(.) f g = \x -> f (g x)
\end{code}
  
Помимо знакомого нам определения и комментариев мы
видим новую прагму \In{INLINE}. Она указывает компилятору
на то, что на этапе упрощения программы необходимо
заменить вызов функции на её правую часть.
Этот процесс называют встраиванием функций.
Замена будет произведена только в случае 
полного применения функции, если синтаксическая арность
(количество аргументов слева от знака равно) совпадает
с числом переданных в функцию аргументов. Поэтому для
GHC есть существенная разница между определениями:

\begin{code}
(.) f g = \x -> f (g x)

(.) f g x = f (g x)
\end{code}
  
Встраиванием функций мы экономим на создании лишних
объектов в куче, но при этом код может существенно
разбухнуть. GHC пользуется эвристическим алгоритмом
при определении когда функцию стоит встраивать,
а когда~-- нет. По умолчанию GHC проводит встраивание
только внутри модуля. Если мы компилируем с флагом \In{O},
функции будут встраиваться между модулями. Для этого
GHC сохраняет в интерфейсном файле (с расширением \In{.hi})
не только типы функций, но и павые части достаточно
кратких функций. Длина функции определяется числом 
узлов в синтаксическом дереве кода её правой части. 
Директивой \In{INLINE} мы приказываем GHC встроить функцию.
Также есть более слабая версия этой прагмы~--\In{INELINABLE}. 
Этой прагмой мы  рекомендуем произвести встраивание 
функции не смотря на её величину.

Задать порог величины функции для встраивания можно 
с помощью флага \In{-funfolding-use-threshold=16}.
Отметим, что если функция не является экспортируемой
и используется лишь один раз, то GHC втроит её в 
любом случае, поэтому стоит определять списки
экспортируемых определений в шапке модуля, иначе
компилятор будет считать, что экспортируются все
определения. 

Прагма \In{INLINE} может стоять в любом месте,
где можно было бы объявить тип значения. 
Так например можно указать компилятору встраивать
методы класса:

\begin{code}
instance Monad T where
    {-# INLINE return #-}
    return = ...
    {-# INLINE (>>=) #-}
    (>>=)  = ...
\end{code}
  
Встраивание значений может существенно ускорить 
программу. Но не стоит венчать каждую экспортируемую функцию 
прагмой \In{INLINE}, возможно GHC встроит их автоматически. 
Посмотреть какие функции были встроены можно по определениям,
попавшим в файл \In{.hi}. 

Например если мы скомпилируем такой код с флагом \In{ddump-hi}:

\begin{code}
module Inline(f, g) where

g :: Int -> Int
g x = x + 2

f :: Int -> Int
f x = g $ g x
\end{code}
  
\noindent то среди прочих определений увидим:

\begin{code}
ghc -c -ddump-hi -O Inline.hs
...
  f :: GHC.Types.Int -> GHC.Types.Int
    {- Arity: 1, HasNoCafRefs, Strictness: U(L)m,
       Unfolding: InlineRule (1, True, False)
                  (\ x :: GHC.Types.Int ->
                   case x of wild { GHC.Types.I# x1 ->
                   GHC.Types.I# (GHC.Prim.+# (GHC.Prim.+# x1 2) 2) }) -}
...
\end{code}
  
В этом виде прочесть функцию не так просто. Ко всем именам 
добавлены имена модулей. Приведём вывод к более простому виду
с помощью флага \In{dsuppress-all}:

\begin{code}
ghc -c -ddump-hi -dsuppress-all -O Inline.hs
...
f :: Int -> Int
    {- Arity: 1, HasNoCafRefs, Strictness: U(L)m,
       Unfolding: InlineRule (1, True, False)
       (\ x :: Int -> case x of wild { I# x1 -> I# (+# (+# x1 2) 2) }) -}
...
\end{code}
 
Мы видим, что все вызовы функции \In{g} были заменены. 
Если вы всё же подозреваете, что GHC не справляется 
с встраиванием ваших часто используемых функций
и это сказывается, попробуйте добавить
к ним \In{INLINE}, но при этом лучше узнать, привело
ли это к росту производительности, проверить 
с помощью профилирования.


\subsection{Прагма RULES}

Разработчики GHC хотели, чтобы их компилятор был расширяемым
и программист мог бы определять специфические для его приложения
правила оптимизации. Для этого была придумана прагма \In{RULES}.
За счёт чистоты функций мы можем в очень простом виде выражать 
инварианты программы. Инвариант~-- это некоторое свойство значения,
которое остаётся постоянным при некоторых преобразованиях.
Наиболее распространённые инварианты имеют собственные
имена. Например, это коммутативность сложения:

\begin{code}
forall a b. a + b = b + a
\end{code}

Здесь мы пишем: для любых \In{a} и \In{b} изменение порядка
следования аргументов у \In{(+)} не влияет на результат. 
С ключевым словом \In{forall} мы уже когда-то встречались,
когда говорили о типе \In{ST}. Помните тип функции \In{runST}?
Пример свойства функции \In{map}:

\begin{code}
forall f g.   map f . map g = map (f . g)
\end{code}

Это свойство принято называть дистрибутивностью. Мы видим,
что функция композиции дистрибутивна относительно функции \In{map}.
Инварианты определяют скрытые закономерности значений. 
За счёт чистоты функций мы можем безболезненно заменить
в любом месте программы левую часть на правую или наоборот.
Оптимизация начинается тогда, когда мы понимаем, что 
одна из частей может быть вычислена гораздо эффективнее
другой. Так в примере с \In{map} выражение справа от
знака равно гораздо эффективнее, поскольку в нём мы 
не строим промежуточный список. Особенно ярко разница
проявляется в энергичной стратегии вычислений. 
Или посмотрим на такое совсем простое свойство:

\begin{code}
map id = id
\end{code}
  
Если мы заменим левую часть на правую, то число сэкономленных
усилий будет пропорционально длине списка. Вряд ли программист
станет писать такие выражения, однако они могут появиться
после выполнения других оптимизаций, например после многих
встраиваний различных функций.

Можно представить, что эти правила являются дополнительными
уравнениями в определении функции:

\begin{code}
map f []        = []
map f (x:xs)    = f x : map f xs

map id a        = a
map f (map g x) = map (f . g) x
\end{code}
  
Словно теперь мы можем проводить сопоставление 
с образцом не только по конструкторам, но и по 
выражениям самого языка и функция \In{map}
стала конструктором. Что интересно, зависимости 
могут быть какими угодно, они могут выражать 
закономерности, присущие той области, которую мы 
описываем. В дополнительных уравнениях мы подставляем
аргументы так же как и в обычных, если где-нибудь
в коде программы находится соответствие с левой частью
уравнения, мы заменяем её на правую. При этом мы 
пишем правила так, чтобы действительно происходила
оптимизация программы, поэтому слева пишется 
медленная версия. 

Такие дополнительные правила пишутся в специальной
прагме \In{RULES}:

\begin{code}
{-# RULES
    "map/compose"   forall f g x.  map f (map g x)  = map (f . g) x    
    "map/id"                       map id           = id 
#-}
\end{code}
  
Первым в кавычках идёт имя правила. Оно используется только для
подсчёта статистики (например если мы хотим узнать сколько
правил сработало в данном прогоне программы). За именем правила
пишут уравнение. В одной прагме может быть несколько уравнений.
Правила разделяются точкой с запятой или переходом на другу строку.
Все свободные переменные правила перечисляются в окружении
\In{forall (...) .}~. Компилятор доверяет нам абсолютно.
Производится только проверка типов. Никаких других проверок
не проводится. Выполняется ли на самом деле это свойство,
будет ли вычисление правой части действительно проще программы 
вычисления левой -- известно только нам. 

Отметим то, что прагма \In{RULES} применяется до тех 
пор пока есть возможность её применять, при этом мы можем
войти в бесконечный цикл:

\begin{code}
{-# RULES
        "infinite"  forall a b. f a b = f b a 
#-}
\end{code}
  

С помощью прагмы \In{RULES} можно реализовать очень сложные 
схемы оптимизации. Так в Prelude реализуется слияние (fusion) списков.
За счёт этой оптимизации многие выражения вида свёртка/развёртка
не будут производить промежуточных списков. Этой схеме будет
посвящена отдельная глава. Например если список
преобразуется серией функций \In{map}, \In{filter} и \In{foldr}
промежуточные списки не строятся. 

Посмотрим как работает прагма \In{RULES}, попробуем 
скомпилировать такой код:

\begin{code}
module Main where

data List a = Nil | Cons a (List a)
    deriving (Show)

foldrL :: (a -> b -> b) -> b -> List a -> b
foldrL cons nil x = case x of
    Nil         -> nil
    Cons a as   -> cons a (foldrL cons nil as) 

mapL :: (a -> b) -> List a -> List b
mapL = undefined

{-# RULES  
"mapL"   forall f xs.
        mapL f xs = foldrL (Cons . f) Nil xs
  #-}

main = print $ mapL (+100) $ Cons 1 $ Cons 2 $ Cons 3 Nil
\end{code}
  
Функция \In{mapL} не определена, вместо этого мы
сделали косвенное определение в прагме \In{RULES}.
Проверим, для того чтобы \In{RULES} заработали, необходимо
компилировать с одним из флагов оптимизаций \In{O} или \In{O2}:

\begin{code}
$ ghc --make -O Rules.hs 
$ ./Rules
Rules: Prelude.undefined
\end{code}

Что-то не так. Дело в том, что GHC слишком поторопился
и заменил простую функцию \In{mapL} на её определение.
Функция \In{$} также очень короткая, если бы нам удалось
задержать встраивание \In{mapL}, тогда \In{$} превратилось
бы в обычное применение и наши правила сработали бы.  

\subsection{Фазы компиляции}

Для решения этой проблемы в прагмы \In{RULES} и \In{INLINE}
были введены ссылки на фазы компиляции. С помощью них мы
можем указать GHC в каком порядке реагировать на эти прагмы.
Фазы пишутся в квадратных скобках:

\begin{code}
{-# INLINE [2] someFun #-}
{-# RULES
"fun" [0] forall ... 
"fun" [1] forall ... 
"fun" [~1] forall ... 
  #-}
\end{code}

Компиляция выполняется в несколько фаз.
Фазы следуют некотрого заданного целого числа, например трёх, до нуля.
Мы можем сослаться на фазу двумя способами: просто номером
и номером с тильдой. Ссылка без тильды говорит: попытайся применить
это правило как можно раньше до тех пор пока не наступит 
данная фаза, далее не применяй. Ссылка с тильдой говорит: 
подожди до наступления этой фазы. В нашем примере
мы задержим встраивание для \In{mapL} и \In{foldrL} так:

\begin{code}
{-# INLINE [1] foldrL #-}
foldrL :: (a -> b -> b) -> b -> List a -> b

{-# INLINE [1] mapL #-}
mapL :: (a -> b) -> List a -> List b
\end{code}

Посмотреть какие правила сработали можно с помощью флага 
\In{ddump-rule-firings}. Теперь скомпилируем:

\begin{code}
$ ghc --make -O Rules.hs -ddump-rule-firings
...
Rule fired: SPEC Main.$fShowList [GHC.Integer.Type.Integer]
Rule fired: mapL
Rule fired: Class op show
...
$ ./Rules 
Cons 101 (Cons 102 (Cons 103 Nil))
\end{code}

Среди прочих правил, определённых в стандартных библиотеках,
сработало и наше. Составим правила, которые будут 
проводить оптимизацию слияние для наших списков, они 
будут заменять последовательное применение
\In{mapL} на один \In{mapL} c композицией функций,
также промежуточный список будет устранён в связке
\In{foldrL/mapL}:

\section{Краткое содержание}

Эта глава была посвящена компилятору GHC. Мы говорим Haskell
подразумеваем GHC, говорим GHC подразумеваем Haskell. 
К сожалению на данный момент у этого компилятора нет достойных
конкурентов. А может и к счастью, ведь если бы не было
GHC, у нас была бы бурная конкуренция среди компиляторов 
поплоше. Мы бы не знали, что они не так хороши. 
Но у нас не было бы программ, которые способны тягаться 
по скорости с С. И мы бы говорили: ну декларативное 
программирование, что поделаешь, за радость абстракций 
приходится платить. Но есть GHC! Всё-таки это очень
трудно: написать компилятор для ленивого языка

Отметим другие компиляторы: Hugs разработан Марком
Джонсом (написан на C), nhc98 основанный Николасом Райомо 
(Niklas Röjemo) этот компилятор задумывался как легковесный
и простой в установке, он разрабатывался при поддержке
NUTEK, Йоркского университета и Технического университета Чалмерса. 
От этого компилятора отпочковался YHC, Йоркский компилятор. 
UHC -- компилятор Утрехтского университета, разработан
для тестирования интересных идей в теории типов. 
JHC (Джон Мичэм,  John Meacham) и LHC 
(Дэвид Химмельступ и Остин Сипп, David Himmelstrup, Austin Seipp)
компиляторы предназначенные для проведения более
сложных оптимизаций программ с помощью преобразований
дерева программы.

В этой главе мы узнали как вычисляются программы в GHC.
Мы узнали об этапах компиляции. Сначала проводится
синтаксический анализ программы и проверка типов,
затем код Haskell переводится на язык Core. Это
сильно урезанная версия Haskell. После этого проводятся
оптимизации, которые преобразуют дерево программы.
На последнем этапе Core переводится на ещё более
низкоуровневый, но всё ещё функциональный язык STG,
который превращается в низкоуровневый код и исполняется
вычислителем. Посмотреть на текст вашей программы
в \In{Core} и \In{STG} можно с помощью флагов 
\In{ddump-simpl} \In{ddump-stg} при этом лучше воспользоваться
флагом \In{ddump-suppress-all} для пропуска многочисленных
деталей. Хардкорные разработчики Haskell смотрят \In{Core}
для того чтобы понять насколько строгой оказалась та
или иная функция, как аргументы размещаются в памяти. 
Но это уже высший пилотаж искусства оптимизации на Haskell.

Мы узнали о том как работает сборщик мусора и научились
просматривать разные параметры работы программы. У нас 
появилось несколько критериев оценки производительности программ:
минимум глубоких очисток и отсутствие горбов на графике 
изменения кучи. Мы потренировались в охоте за утечками памяти
и посмотрели как разные типы профилирования могут подсказать
нам в каком месте затаилась ошибка. Отметим, что не стоит 
в каждой медленной программе искать утечку памяти. Так
в примере \In{concat} у нас не было утечек памяти,
просто один из алгоритмов работал очень плохо и
через профилирование функций мы узнали какой. 

Также мы познакомились с новыми прагмами оптимизации
программ. Это встраиваемые функции \In{INLINE} 
и правила преобразования выражений \In{RULE}. 
Разработчики GHC отмечают, что грамотное использование
прагмы \In{INLINE} может существенно повысить 
скорость программы. Если мы встраиваем функцию, которая 
используется очень часто, нам не нужно создавать лишних
отложенных вычислений при её вызовах. 

Надеюсь, что содержание этой главы упростит понимание
программ. Как они вычисляются, куда идёт память,
почему она висит в куче. При оптимизации программ
предпочитайте изменение алгоритма перед настройкой 
параметров компилятора под плохой алгоритм. 
Вспомните самый первый пример, увеличением памяти
под сборку мусора нам удалось вытянуть ленивую
версию \In{sum}, но ведь строгая версия требовала
в 100 раз меньше памяти, причём её запросы не зависели
от величины списка. Если бы мы остановились на ленивой
версии, вполне могло бы так статься, что первый год
нас бы устраивали результаты, но потом
наши аппетиты могли возрасти. И вдруг программа,
так тщательно настроенная, взорвалась. За год
мы, конечно, многое позабыли о её внутренностях,
искать ошибку было бы гораздо труднее. Впрочем
не так безнадёжно: включаем \In{auto-all}, \In{caf-all}
с флагом \In{prof} и смотрим отчёт после флага \In{p}.

    
\section{Упражнения}

\begin{itemize}

\item Попытайтесь понять причину утечки памяти в примере с
функцией \In{sum2} на уровне STG. Не запоминайте этот пример,
вроде, ага, тут у нас копятся отложенные вычисления в аргументе.
Переведите на STG и посмотрите в каком месте происходит слишком
много вызовов \In{let}-выражений. Переведите и пример без
утечки памяти, а также промежуточный вариант, который не сработал.
Для этого вам понадобится выразить энергичный образец через
функцию \In{seq}. 

Подсказка: За счёт семантики \In{case}-выражений
нам не нужно специальных конструкций для
того чтобы реализовать \In{seq} в STG:

\begin{code}
seq = FUN( a b ->
        case a of
            x -> b
      )
\end{code}
  
При этом вызов функции \In{seq} будет встроен. Необходимо
будет заменить в коде все вызовы \In{seq} на правую часть определения
(без \In{FUN}). Также обратите внимание на то, что плюс не
является примитивной функцией:


\begin{code}
plusInt = FUN( ma mb ->
            case ma of
                I# a -> case mb of
                            I# b -> case (primitivePlus a b) of
                                        res -> I# res
          )
\end{code}

В этой функции всплыла на поверхность одна тонкость. 
Если бы мы писали это выражение в Haskell, то мы бы сразу
вернули результат \In{(I}\verb!#!\In{ (primitivePlus a b))}, но
мы пишем в STG и конструктор может принять только атомарное выражение.
Тогда мы могли бы подумать и сохранить его по старинке в 
\In{let}-выражении:

\begin{code}
-> let v = primitivePlus a b
   in  I# v 
\end{code}
  
Но это не правильное выражение в STG! Конструкция в правой части
\In{let}-выражения должна быть объектом кучи, а у нас там простое
выражение. Но было бы плохо добавить к нему \In{THUNK},
поскольку это выражение содержит вызов примитивной функции
на незапакованных значениях. Эта операция выполняется очень быстро.
Было бы плохо создавать для неё специальный объект
на куче. Поэтому мы сразу вычисляем это выражение в третьем \In{case}.
Эта функция также будет встроенной, необходимо заменить все
вызовы на определение. 

\item Набейте руку в профилировании, пусть это станет привычкой.
Вы долго писали большую программу и теперь вы можете
узнать много подробностей из её жизни, что происходит с ней
во время вычисления кода. Вернитесь к прошлой главе и 
попрофилируйте разные примеры. В конце главы мы рассматривали
пример с поиском корней, там мы создавали большой список
промежуточных результатов и в нём искали решение.
Я говорил, что такие алгоритмы очень эффективны при 
ленивой стратегии вычислений, но так ли это? Будьте
критичны, не верьте на слово, ведь теперь у вас есть 
инструменты для проверки моих туманных гипотез.

\item Откройте документацию к GHC. Пролистайте её. 
Проникнитесь уважением к разработчикам GHC. 
Найдите исходники GHC и почитайте их. Посмотрите на Haskell-код,
написанный профессионалами. Выберите функцию
наугад и попытайтесь понять как она строит свой результат.


\item Откройте документацию вновь. Нас интересует
глава \In{Profiling}. Найдите в разделе профилирование
кучи как выполняется retainer profiling. Это специальный
тип профилирования направленный на поиск данных, которые
удерживают в памяти другие данные (типичный сценарий для
утечек памяти). Разберитесь с этим типом профилирования 
(флаг \In{hr}).

\item Постройте систему правил, которая выполняет 
слияние для списков  \In{List}, определённых в примере
для прагмы \In{RULES}. Сравните показатели производительности
с правилами и без (для этого скомпилируйте дважды с флагом \In{O} и без) 
на тестовом выражении:

\begin{code}
main = print $ sumL $ 
    mapL (\x -> x - 1000) $ mapL (+100) $ mapL (*2) $ genL 0 1e6
\end{code}

Функция \In{sumL} находит сумму элементов в списке, 
функция \In{genL} генерирует список чисел с единичным
шагом от первого аргумента до второго. 

Подсказка: вам нужно воспользоваться такими свойствами
(не забудьте о фазах компиляции)

\begin{code}
mapL f (mapL g xs)              = ...
foldrL cons nil (mapL f xs)     = ...     
\end{code}

\item Откройте исходный код \In{Prelude} и присмотритесь
к различным прагмам. Попытайтесь понять почему они там 
используются.

\end{itemize}

